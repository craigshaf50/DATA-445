{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d5257e",
   "metadata": {},
   "source": [
    "**Exercise 1: A data scientist is running an AdaBoost classifier on a dataset with 100 observations. Answer the following:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e894ed2",
   "metadata": {},
   "source": [
    "(a) (3 points) What is the weight initial weight of observation 72th in the training dataset? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae8511",
   "metadata": {},
   "source": [
    "   The initial weight is 1/100. This is because initial weights are equal to 1/M, where M is the number of observations. All observations are weighted equally initially but are updated based on if they are misclassified or not. Misclassified observations will have a higher weight, so that it gets more attention while correct classifications recieve lower weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cbc62e",
   "metadata": {},
   "source": [
    "(b) (3 points) The 72th observation in the training dataset is misclassified by the first weak learner chosen by the data scientist. Is the new weight of the 72th observation in the training dataset (i.e., the weight assigned to the 72th observation after choosing the first weak classifier) larger or smaller than the weight assigned to that observation initially? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c56c9",
   "metadata": {},
   "source": [
    "Since it was misclassified, it will have a larger weight so it will get more attention in the sequence. If it were classified correctly, it would get a lower weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd8804",
   "metadata": {},
   "source": [
    "**Exercise 2: (4 points) Explain why AbaBoost.M1 is an ensemble learning algorithm? Be specific.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09518632",
   "metadata": {},
   "source": [
    "Adaboost.M1 is an ensemble learning algorithm because it trains trees in a series. With adaboost.M1, a set of weak classifiers is connected in series such that each weak classifier tries to improve the classification of observations that were misclassified by the previous weak classifier. In doing so, boosting combines weak classifiers in series to create a strong classifier. Since it uses multiple trees, it is an ensemble learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58890e7e",
   "metadata": {},
   "source": [
    "**Exercise 3: (10 points) Suppose you are running AdaBoost.M1 (with Î· = 0.1) with 4 training examples. At the start of the current iteration, the four examples have the weights shown in the following table. Another column says if the weak classifier got them correct or incorrect. Determine the new\n",
    "weights for these four examples, and fill in the corresponding entries in the table. Show all your work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439f1f8",
   "metadata": {},
   "source": [
    "refer to pdf submission (PNG has correct answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f91f07",
   "metadata": {},
   "source": [
    "**Exercise 4: (4 points) If your AdaBoost ensemble under-fits the training dataset, what would you do to fix that? That is, which hyper-parameters should you tweak?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4de3a",
   "metadata": {},
   "source": [
    "You could increase the number of estimators (n_estimators) and increase the learning rate (learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583075a0",
   "metadata": {},
   "source": [
    "**Exercise 5: (4 points) For binary classification, which of the following statements are TRUE of AdaBoost with decision trees as learners?**\n",
    " - **(a) It usually has lower bias than a single decision tree.**\n",
    " - **(b) It is popular because it usually works well even before any hyper-parameter tuning.**\n",
    " - **(c) It assigns higher weights to observations that have been misclassified.**\n",
    " - **(d) It can train multiple decision trees in parallel.**\n",
    " - **(e) All of the above.**\n",
    " - **(f) None of the above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa02a47",
   "metadata": {},
   "source": [
    "A, B, & C\n",
    "\n",
    "Thought process: C is true and A is true. I've read adaboost is one of the best \"out of the box\" and \"less susceptible to overfitting than other algorithms\" which leads me to believe that B is true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9446be1",
   "metadata": {},
   "source": [
    "**Exercise 6: (4 points) Which of the following is/are TRUE about gradient boosting trees?**\n",
    " - **(a) In gradient boosting trees, the decision trees are independent of each other.**\n",
    " - **(b) In gradient boosting trees, the decision trees are dependent of each other.**\n",
    " - **(c) It is a method for improving the performance by aggregating the results of several decision trees.**\n",
    " - **(d) (a) and (b)**\n",
    " - **(e) (a) and (c)**\n",
    " - **(f) (b) and (c)**\n",
    " - **(g) None of the above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa9bb5",
   "metadata": {},
   "source": [
    "(f) b and c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668c13e",
   "metadata": {},
   "source": [
    "**Exercise 7: (6 points) In this course have covered two boosting frameworks. What is the main difference between AdaBoost and Gradient Boosting? Be specific.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c287e3",
   "metadata": {},
   "source": [
    "Gradient boosting trains the learners and reduces the weak learners' loss functions based on the model's residuals and Adaboost focuses on training based on prior misclassified observations by weighting them higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fc219",
   "metadata": {},
   "source": [
    "**Exercise 8: Consider the framingham.csv data file. The dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD). The dataset provides the patients? information. It includes over 4,000 records and 15 attributes. Each attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors. In Python, answer the following:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e56f6",
   "metadata": {},
   "source": [
    "(a) (5 points) Using the pandas library, read the csv data file and create a data-frame called\n",
    "heart. Remove the observations with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3e3775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_column', 100)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, accuracy_score \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,  GradientBoostingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Defining the s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'craig-shaffer-data-445-bucket'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the file to be read from s3 bucket\n",
    "file_key = 'framingham.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "# reading the datafile\n",
    "heart = pd.read_csv(file_content_stream)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3b0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping observations w/ missing values\n",
    "heart=heart.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef1fab",
   "metadata": {},
   "source": [
    "(b) (45 points) Based on the different interactions that we have so far with this data file, it seems that age, totChol, sysBP, BMI, heartRate, and glucose are the most important predictor variables when it comes to predict the likelihood of TenYearCHD. Then do the following:\n",
    "- (i) Using TenYearCHD as the target variable and the other variables as the input variables, split the data into two data-frames (taking into account the proportion of 0s and 1s): train (80%) and test (20%).\n",
    "- (ii) Using the train data-frame, build an random forest model (with 500 trees and max tree depth equal 3) in which: age, totChol, sysBP,BMI, heartRate, and glucose are the predictor variables, and TenYearCHD is the target variable. Using this model, predict the risk of coronary disease of the patients in the test data-frame. Using 10% as cutoff value, report the accuracy and recall.\n",
    "- (iii) Using the train data-frame, build an AdaBoost model (with 500 trees, max tree depth equal 3, and learning rate equal to 0.01) in which: age, totChol, sysBP,BMI, heartRate, and glucose are the predictor variables, and TenYearCHD is the target variable. Using this model, predict the risk of coronary disease of the patients in the test data-frame. Using 10% as cutoff value, report the accuracy and recall.\n",
    "- (iv) Using the train data-frame, build an gradient boosting model (with 500 trees, max tree depth equal 3, and learning rate equal to 0.01) in which: age, totChol, sysBP,BMI, heartRate, and glucose are the predictor variables, and TenYearCHD is the target variable. Using this model, predict the risk of coronary disease of the patients in the test data-frame. Using 10% as cutoff value, report the accuracy and recall.\n",
    "\n",
    "Repeat (i)-(iv) 100 times and compute the average accuracy and average recall for each of the model. What model would you use to predict TenYearCHD based on the accuracy and recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6ff481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the input and target variables\n",
    "x = heart[['age','totChol','sysBP','BMI','heartRate','glucose']]\n",
    "y = heart['TenYearCHD']\n",
    "\n",
    "rf_recall = list()\n",
    "ada_recall = list()\n",
    "gb_recall = list()\n",
    "\n",
    "rf_accuracy = list()\n",
    "ada_accuracy = list()\n",
    "gb_accuracy = list()\n",
    "\n",
    "for i in range (0,100):\n",
    "    #splitting the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y)\n",
    "    \n",
    "    #random forest model\n",
    "    rf_md = RandomForestClassifier(n_estimators = 500, max_depth = 3).fit(x_train, y_train)\n",
    "    #predict on test\n",
    "    rf_pred = rf_md.predict_proba(x_test)[:,1]\n",
    "    #changing likelihoods to labels w/ 10% cutoff\n",
    "    rf_labels = np.where(rf_pred < .1, 0, 1)\n",
    "    rf_recall.append(recall_score(y_test, rf_labels))\n",
    "    rf_accuracy.append(accuracy_score(y_test, rf_labels))\n",
    "    \n",
    "    #adaboost model\n",
    "    ada_md = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 3), n_estimators = 500, learning_rate =.01).fit(x_train, y_train)\n",
    "    #predict on test\n",
    "    ada_pred = ada_md.predict_proba(x_test)[:,1]\n",
    "    #changing likelihoods to labels w/ 10% cutoff\n",
    "    ada_labels = np.where(ada_pred < .1, 0, 1)\n",
    "    ada_recall.append(recall_score(y_test, ada_labels))\n",
    "    ada_accuracy.append(accuracy_score(y_test, ada_labels))\n",
    "    \n",
    "    #gradient boost model\n",
    "    gb_md= GradientBoostingClassifier(max_depth = 3, n_estimators = 500, learning_rate =.01).fit(x_train, y_train)\n",
    "    #predict on test\n",
    "    gb_pred = gb_md.predict_proba(x_test)[:,1]\n",
    "    #changing likelihoods to labels w/ 10% cutoff\n",
    "    gb_labels = np.where(gb_pred < .1, 0, 1)\n",
    "    gb_recall.append(recall_score(y_test, gb_labels))\n",
    "    gb_accuracy.append(accuracy_score(y_test, gb_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc22115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg recall for the random forest model: 0.8575\n",
      "the avg recall for the adaboost model: 0.991875\n",
      "the avg recall for the gradient boost model: 0.8067857142857144\n",
      "the avg accuracy for the random forest model: 0.4508743169398908\n",
      "the avg accuracy for the adaboost model: 0.15874316939890712\n",
      "the avg accuracy for the gradient boost model: 0.5042622950819672\n"
     ]
    }
   ],
   "source": [
    "print('the avg recall for the random forest model:', np.mean(rf_recall))\n",
    "print('the avg recall for the adaboost model:',np.mean(ada_recall))\n",
    "print('the avg recall for the gradient boost model:',np.mean(gb_recall))\n",
    "\n",
    "print('the avg accuracy for the random forest model:', np.mean(rf_accuracy))\n",
    "print('the avg accuracy for the adaboost model:',np.mean(ada_accuracy))\n",
    "print('the avg accuracy for the gradient boost model:',np.mean(gb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6c6fc",
   "metadata": {},
   "source": [
    "Though adaboost's recall is very high (.99), its accuracy score is very low (.16). Because of the accuracy, it cannot be considered the best model. That left me with the options of random forest (which is the middle option for both accuracy and recall) or gradient boost (highest accuracy and lowest recall but still close to random forest). Ultimately, the choice came down to higher recall or accuracy. Since we are dealing with disease, we don't want false negatives. Therefore, we'd rather the model with higher recall which is random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d944be2",
   "metadata": {},
   "source": [
    "(c) (7 points) Assuming that the ideal model needs to have a minimum accuracy and recall equal to 80%. Does the best model from part (b) meet this requirement? If not, provide recommendations how you tweak the model to reach the minimum requirements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261dbb4",
   "metadata": {},
   "source": [
    "No, in fact, none of the models meet the requirement of accuaracy greater than or equal to 80%. In the case of the random forest model, we should try to tune our parameters like number of trees (n_estimators), how deep our trees are (max_depth), among other variables to improve the accuracy to 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0924a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
