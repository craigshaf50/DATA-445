{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c39aad6",
   "metadata": {},
   "source": [
    "**Exercise 1: (5 points) If a decision tree is under-fitting the training dataset, is it a good idea to try scaling\n",
    "the input features?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7eedf0",
   "metadata": {},
   "source": [
    "No, because there is no need for scaling in decision tree models because it does not affect the output. It would not solve the problem of over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a36e11",
   "metadata": {},
   "source": [
    "**Exercise 2: (5 points) If a decision tree is over-fitting the training dataset, is it a good idea to try decreasing\n",
    "max depth?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183d309",
   "metadata": {},
   "source": [
    "Yes, reducing the max depth will help regularize the model and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b69ab",
   "metadata": {},
   "source": [
    "**Exercise 3: (4 points) Why would you use a random forest instead of a decision tree?**\n",
    "- **(a) For a lower training error.**\n",
    "- **(b) To reduce the variance of the model.**\n",
    "- **(c) For a model that it is easier for human to interpret.**\n",
    "- **(d) (a) and (b)**\n",
    "- **(e) (a) and (c)**\n",
    "- **(f) (b) and (c)**\n",
    "- **(g) All of the above**\n",
    "- **(h) None of the above**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37fb21",
   "metadata": {},
   "source": [
    "(d) a and b\n",
    "\n",
    "It reduces error and variance. However, it dones not become easier to interpret because it is multiple decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d3e5b",
   "metadata": {},
   "source": [
    "**Exercise 4: (4 points) Which of the following is/are TRUE about bagging trees?**\n",
    "- **(a) In bagging trees, the trees are grown independent of each other.**\n",
    "- **(b) In bagging trees, the trees are grown in sequence.**\n",
    "- **(c) Bagging is a method for improving the performance by aggregating the results of weak learners.**\n",
    "- **(d) (a) and (c)**\n",
    "- **(e) (b) and (c)**\n",
    "- **(f) None of the above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c376afd",
   "metadata": {},
   "source": [
    "(d) a and c\n",
    "\n",
    "In bagging, trees are grown independently and bagging is a method for improving the performance by aggregating the results of weak learners.\n",
    "(note: boosting involves growing trees sequentially.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a53ff",
   "metadata": {},
   "source": [
    "**Exercise 5: (12 points) Suppose you are building random forest model, which split a node on the attribute,\n",
    "that has highest information gain (using the Gini index). In the below image, which attribute\n",
    "which has the highest information gain? Show all your calculations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06994386",
   "metadata": {},
   "source": [
    "You would choose \"outlook\" because it had the lowest weighted sum of Gini indices (roughly 0.34) and therefore, it offers the highest information gain.\n",
    "\n",
    "refer to pdf submission for calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fe729",
   "metadata": {},
   "source": [
    "**Exercise 6: Consider the framingham.csv data file. The dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD). The dataset provides the patients? information. It includes over 4,000 records and 15 attributes. Each attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors. In Python, answer the following:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada11ad1",
   "metadata": {},
   "source": [
    "(a) (4 points) Load the data file to you S3 bucket. Using the pandas library, read the csv data\n",
    "file and create a data-frame called heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6837032d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_column', 100)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "## Defining the s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'craig-shaffer-data-445-bucket'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the file to be read from s3 bucket\n",
    "file_key = 'framingham.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "# reading the datafile\n",
    "heart = pd.read_csv(file_content_stream)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b46fc",
   "metadata": {},
   "source": [
    "(b) (3 points) Remove observations with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fa919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping observations w/ missing values\n",
    "heart=heart.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df440a15",
   "metadata": {},
   "source": [
    "(c) (30 points) Using all the available variables as the predictor variables, and TenYearCHD as\n",
    "the target variable, do the following:\n",
    "- (i) Split the data into train (80%) and test (20%) (taking into account the proportion of 0s and 1s).\n",
    "- (ii) Using the train data-frame, build a random forest classifier (using 500 trees).\n",
    "- (iii) Extract the feature importance of each of the variables.\n",
    "\n",
    "Repeat (i)-(iii) 100 times. Compute the average importance of each of the variables across\n",
    "the 100 splits. After that, select the top 5 variables (the ones with top 5 average importance)\n",
    "as the predictor variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70035c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = list()\n",
    "\n",
    "for i in range (0,100):\n",
    "\n",
    "    # defining the input and target variables\n",
    "    x = heart.drop(columns = ['TenYearCHD'],axis=1)\n",
    "    y = heart['TenYearCHD']\n",
    "\n",
    "    #splitting the data\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = .2,stratify=y)\n",
    "\n",
    "    #build the model\n",
    "    RF_md = RandomForestClassifier(n_estimators = 500).fit(x_train,y_train)\n",
    "\n",
    "    #extract the feature importances\n",
    "    importance.append(RF_md.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28dd4a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sysBP</th>\n",
       "      <td>0.134887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.127056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.124192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totChol</th>\n",
       "      <td>0.122082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glucose</th>\n",
       "      <td>0.120016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diaBP</th>\n",
       "      <td>0.118947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heartRate</th>\n",
       "      <td>0.096322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cigsPerDay</th>\n",
       "      <td>0.050477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.036895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.021208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalentHyp</th>\n",
       "      <td>0.018332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currentSmoker</th>\n",
       "      <td>0.012614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BPMeds</th>\n",
       "      <td>0.007015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>0.006620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalentStroke</th>\n",
       "      <td>0.003336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Importance\n",
       "sysBP              0.134887\n",
       "BMI                0.127056\n",
       "age                0.124192\n",
       "totChol            0.122082\n",
       "glucose            0.120016\n",
       "diaBP              0.118947\n",
       "heartRate          0.096322\n",
       "cigsPerDay         0.050477\n",
       "education          0.036895\n",
       "male               0.021208\n",
       "prevalentHyp       0.018332\n",
       "currentSmoker      0.012614\n",
       "BPMeds             0.007015\n",
       "diabetes           0.006620\n",
       "prevalentStroke    0.003336"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = pd.DataFrame(importances)\n",
    "importance.columns = ['male','age','education','currentSmoker','cigsPerDay','BPMeds','prevalentStroke','prevalentHyp','diabetes','totChol','sysBP','diaBP','BMI','heartRate','glucose']\n",
    "i = pd.DataFrame({'Importance':importance.apply(np.mean, axis = 0)})\n",
    "i.sort_values(by = 'Importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b350d",
   "metadata": {},
   "source": [
    "    The top 5 variables are sysBP, BMI, age, totChol, and glucose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef4df7",
   "metadata": {},
   "source": [
    "(d) (35 points) Using the top 5 variables from part (c) as the predictor variables and TenYearCHD as the target variable, do the following:\n",
    "- (i) Split the data into train (80%) and test (20%) (taking into account the proportion of 0s and 1s).\n",
    "- (ii) Using the train data-frame, build a random forest classifier (using 500 trees and maximum depth tree equal to 3). Using this model, predict the likelihood of risk of coronary disease of the patients in the test data-frame. Using 10% as cutoff value, report the recall.\n",
    "- (iii) Using the train data-frame, build a random forest classifier (using 500 trees and maximum depth tree equal to 5). Using this model, predict the likelihood of risk of coronary disease of the patients in the test data-frame. Using 10% as cutoff value, report the recall.\n",
    "- (iv) Using the train data-frame, build a random forest classifier (using 500 trees and maximum depth tree equal to 7). Using this model, predict the likelihood of risk of coronary disease of the patients in the test data-frame. Using 10% as cutoff value, report the recall.\n",
    "\n",
    "Repeat (i)-(iii) 100 times. Compute the average recall of each of the models across the 100\n",
    "iterations. What model would use to predict TenYearCHD? Be specific.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82365136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists to store recall values for each model\n",
    "md1_results= list()\n",
    "md2_results= list()\n",
    "md3_results= list()\n",
    "\n",
    "#defining the input (top 5 features) and target for RF\n",
    "x = heart[['sysBP', 'BMI', 'age', 'totChol', 'glucose']]\n",
    "y = heart['TenYearCHD']\n",
    "    \n",
    "for i in range (0,100):\n",
    "\n",
    "    #splitting the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, stratify=y)\n",
    "\n",
    "    \n",
    "    #RF model with max_depth = 3\n",
    "    RF_md1 = RandomForestClassifier(max_depth = 3, n_estimators = 500).fit(x_train, y_train)\n",
    "    \n",
    "    #predict on test\n",
    "    RF_pred1 = RF_md1.predict_proba(x_test)[:,1]\n",
    "    \n",
    "    #changing likelihoods to labels w/ 10% cutoff\n",
    "    RF_label1 = np.where(RF_pred1 < .1, 0, 1)\n",
    "    \n",
    "    #computing the recall\n",
    "    RF_recall1 = recall_score(y_test, RF_label1)\n",
    "    md1_results.append(RF_recall1)\n",
    "\n",
    "    \n",
    "\n",
    "    #RF model with max_depth = 5\n",
    "    RF_md2 = RandomForestClassifier(max_depth = 5, n_estimators = 500).fit(x_train, y_train)\n",
    "    \n",
    "    #predict on test\n",
    "    RF_pred2 = RF_md2.predict_proba(x_test)[:,1]\n",
    "    \n",
    "    #changing likelihoods to labels w/ 10% cutoff\n",
    "    RF_label2 = np.where(RF_pred2 < .1, 0, 1)\n",
    "    \n",
    "    #computing the recall\n",
    "    RF_recall2 = recall_score(y_test, RF_label2)\n",
    "    md2_results.append(RF_recall2)\n",
    "\n",
    "    \n",
    "\n",
    "    #RF model with max_depth = 7\n",
    "    RF_md3 = RandomForestClassifier(max_depth = 7, n_estimators = 500).fit(x_train, y_train)\n",
    "    \n",
    "    #predict on test\n",
    "    RF_pred3 = RF_md3.predict_proba(x_test)[:,1]\n",
    "\n",
    "    #changing likelihoods to labels w/ 10% cutoff\n",
    "    RF_label3 = np.where(RF_pred3 < .1, 0, 1)\n",
    "    \n",
    "    #computing the recall\n",
    "    RF_recall3 = recall_score(y_test, RF_label3)\n",
    "    md3_results.append(RF_recall3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf713913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg recall for model 1 (max_depth=3): 0.8414285714285715\n",
      "the avg recall for model 2 (max_depth=5): 0.8230357142857143\n",
      "the avg recall for model 3 (max_depth=7): 0.8095535714285714\n"
     ]
    }
   ],
   "source": [
    "print('the avg recall for model 1 (max_depth=3):', np.mean(md1_results))\n",
    "print('the avg recall for model 2 (max_depth=5):',np.mean(md2_results))\n",
    "print('the avg recall for model 3 (max_depth=7):',np.mean(md3_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd05eb",
   "metadata": {},
   "source": [
    "Based on the results above, I would use the first model (max_depth=3) because it had the highest avg recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
