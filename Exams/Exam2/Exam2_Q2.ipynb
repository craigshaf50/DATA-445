{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb03eca",
   "metadata": {},
   "source": [
    "**2. Consider the churn-bigml-80.csv and churn-bigml-20.csv datafile for this question. The Orange Telecom’s churn dataset, which consists of cleaned customer activity data (features), along with a churn label specifying whether a customer canceled the subscription, will be used to develop predictive models. Each row represents a customer; each column contains customer’s attributes. The goal is to build models that can help Orange Telecom to flag customers who likely to churn.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102145f2",
   "metadata": {},
   "source": [
    "**(a) (5 points) Load the data files to your S3 bucket. Using the pandas library, read the csv data file and create two data-frames called: telecom train (for churn-bigml-80.csv) and telecom test (for churn-bigml-20.csv).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917594ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "951a5d5d",
   "metadata": {},
   "source": [
    "**(b) (12 points) Conduct the following feature engineering:**\n",
    "- **Using the numpy library, create a variable in telecom_train called Churn_numb that takes the value of 1 when Churn = True and 0 when Churn = False.**\n",
    "- **Change the International_plan variable from a categorical variable to a numerical variable. That is, change Yes to 1 and No to 0 in both data-frames: telecom_train and telecom_test.**\n",
    "- **Change the Voice_mail_plan variable from a categorical variable to a numerical variable. That is, change Yes to 1 and No to 0 in both data-frames: telecom_train and telecom_test.**\n",
    "- **Create a new variable called: total_charge as the sum of Total_day_charge, Total_eve_charge, Total_night_charge, and Total_intl_charge in both data-frames: telecom_train and telecom_test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a65ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d887f8ad",
   "metadata": {},
   "source": [
    "**(c) (5 points) In both data-frames telecom train and telecom test, only keep the following variables: Account_length, International_plan, Voice_mail_plan, total_charge, Customer_service_calls, and Churn_numb.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4c8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e65012ab",
   "metadata": {},
   "source": [
    "**(d) (20 points) Consider the telecom train dataset. Using Account length, International plan, Voice mail plan, total charge, and Customer service calls as the input variables, and Churn is the target variable. Do the following:**\n",
    "- **(1) Split the data into train (80%) and test (20%) taking into account the proportion of 0s and 1s in the data. That is, if Y is the target variable, in train test split function, you need to add the extra argument stratify = Y.**\n",
    "- **(2) Using the train dataset:**\n",
    "    - **(i) Fit a random forest model with 500 trees and depth equal to 3 to the train dataset. Extract the importance of variables.**\n",
    "    - **(ii) Fit an AdaBoost model with 500 trees, depth equal to 3, and learning rate equal to 0.01 to the train dataset. Extract the importance of variables.**\n",
    "    - **(iii) Fit a gradient boosting model with 500 trees, depth equal to 3, and learning rate equal to 0.01 to the train dataset. Extract the importance of variables.**\n",
    "    \n",
    "**Repeat steps (1)-(2) 1000 times. Compute the average importance of each of the vari\u0002ables across the 1000 splits and the three models. After that, select the top 4 variables (the ones with top 4 average importance) as the predictor variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713468ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec92e415",
   "metadata": {},
   "source": [
    "**(e) (45 points) Consider the telecom train dataset. Using Churn as the target variable, and the remaining variables as the input variables. Do the following:**\n",
    " - **(i) Split the data into train (80%) and test (20%) taking into account the proportion of 0s and 1s in the data. That is, if Y is the target variable, in train test split function, you need to add the extra argument stratify = Y.**\n",
    " - **(ii)**\n",
    "   - **Using the train dataset, build random forest models with the following setting: n_tree = [100, 500, 1000, 1500, 2000] and depth = [3, 5, 7]. In order to create a data-frame that contains all the combinations of trees and depths, you can use the following code: (refer to pdf) For each random forest model that is built, use it to predict the likelihood of churn on the test dataset. Using 10% as the cut-off value, compute the accuracy and recall of each of the models.**\n",
    "   - **Using the train dataset, build AdaBoost models with the following setting: n_tree = [100, 500, 1000, 1500, 2000], depth = [3, 5, 7], and learning rate = [0.1, 0.01, 0.001]. In order to create a data-frame that contains all the combinations of trees, depths, and learning rates, you can use the following code: (refer to pdf) For each AdaBoost model that is built, use it to predict the likelihood of churn on the test dataset. Using 10% as the cut-off value, compute the accuracy and recall of each of the models.**\n",
    "   - **Using the train dataset, build gradient boosting models with the following setting: n_tree = [100, 500, 1000, 1500, 2000], depth = [3, 5, 7], and learning rate = [0.1, 0.01, 0.001]. In order to create a data-frame that contains all the combinations of trees, depths, and learning rates, you can use the following code: (refer to the pdf) For each gradient boosting model that is built, use it to predict the likelihood of churn on the test dataset. Using 10% as the cut-off value, compute the accuracy and recall of each of the models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03f1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afe318eb",
   "metadata": {},
   "source": [
    "**(f) (30 points) Repeat part (e) 100 times. Identify the best model of each of the frameworks (based on the average accuracy and recall); that is, identify the best random forest model, the best AdaBoost model, and the best gradient boosting model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8aa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24877f80",
   "metadata": {},
   "source": [
    "**(g) (35 points) Using the telecom train build three models: the best random forest model from part (f), the best AdaBoost model form part (f), and the best gradient boosting model form part (f). Using these to three models, predict the likelihood of Churn on the telecom test data-frame. After that, aggregate those likelihoods using the weighted average formula (use average recall of the models as weights). Using 10% as cutoff value, report the accuracy and recall of the aggregated predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93016d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
